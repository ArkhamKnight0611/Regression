{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that uses regularization to improve prediction accuracy and interpretability. Unlike standard linear regression, Lasso adds a penalty term to the objective function that encourages sparsity. This means some coefficients of the model are shrunk to zero, effectively selecting only the most important features for prediction.\n",
        "\n",
        "Key differences:\n",
        "\n",
        "Variable selection: Lasso performs automatic feature selection, unlike standard regression.\n",
        "Sparsity: Lasso encourages fewer non-zero coefficients, leading to simpler models.\n",
        "Regularization: Lasso penalizes large coefficients, reducing overfitting compared to standard regression.\n",
        "Ridge Regression: Another regularization technique, Ridge shrinks all coefficients towards zero, but doesn't set any to zero, making it less effective for feature selection.\n",
        "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "\n",
        "The main advantage is automatic feature selection. It identifies the most relevant features that contribute to the prediction, leading to:\n",
        "\n",
        "Simpler models: Easier to interpret and less prone to overfitting.\n",
        "Reduced computational cost: Fewer features require less processing power.\n",
        "Improved understanding of data: Helps identify key drivers of the target variable.\n",
        "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "\n",
        "Similar to standard regression, coefficients represent the impact of each feature on the target variable. However, in Lasso:\n",
        "\n",
        "Coefficients can be zero, indicating no impact.\n",
        "Non-zero coefficients have magnitudes that reflect their relative importance.\n",
        "Interpret the effect of non-zero coefficients as in standard regression.\n",
        "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
        "\n",
        "Lambda: Controls the regularization strength. Higher lambda shrinks more coefficients towards zero, leading to:\n",
        "Simpler model: Reduced overfitting but potentially losing important features.\n",
        "Higher bias: Less accurate predictions.\n",
        "Finding the optimal lambda value is crucial for balancing bias and variance.\n",
        "Q5. Can Lasso Regression be used for non-linear regression problems?\n",
        "\n",
        "Lasso cannot directly handle non-linearity. However, you can:\n",
        "\n",
        "Transform features: Create non-linear combinations of features to capture non-linear relationships.\n",
        "Use ensemble methods: Combine Lasso with other algorithms like tree-based methods that handle non-linearity.\n",
        "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "\n",
        "Both penalize coefficients. However:\n",
        "\n",
        "Lasso: Uses L1 norm (absolute value sum) penalty, leading to sparse models with possible zero coefficients.\n",
        "Ridge: Uses L2 norm (squared sum) penalty, shrinking all coefficients towards zero but not setting any to zero.\n",
        "Lasso is better for feature selection, while Ridge might be better when features are highly correlated.\n",
        "Q7. Can Lasso Regression handle multicollinearity in the input features?\n",
        "\n",
        "Multicollinearity (correlated features) can affect both methods. However, Lasso is more robust because:\n",
        "\n",
        "Sparsity encourages selecting only one representative from correlated features.\n",
        "Ridge tends to spread the impact across correlated features, potentially losing interpretability.\n",
        "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
        "\n",
        "Several methods exist:\n",
        "\n",
        "Cross-validation: Train the model on different subsets of data with varying lambda values and choose the one with the best performance on unseen data.\n",
        "Information criteria: Use metrics like AIC or BIC that penalize model complexity along with prediction error to select the optimal lambda."
      ],
      "metadata": {
        "id": "mCsoyfYA6roc"
      }
    }
  ]
}